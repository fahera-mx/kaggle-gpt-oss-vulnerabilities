{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bc9597-a5aa-45c4-8329-3b478a38fa40",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ef84a2-ab1b-4d20-a6e8-eef340312ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.traits import (\n",
    "    VULNERABLE_TRAITS, \n",
    "    NONVULNERABLE_TRAITS, \n",
    "    DEMOGRAPHIC_TRAITS\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83da834a-a6e4-47f0-8809-071a53680417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18beff96-8ba5-48d1-b168-429c0f04d74e",
   "metadata": {},
   "source": [
    "## Initial config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d8209c3-251a-4608-8856-5a29f2418839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env keys\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "base_url = os.getenv('BASE_URL')\n",
    "\n",
    "# constants\n",
    "temperature = 1\n",
    "top_p = 1\n",
    "max_tokens = 4096\n",
    "stream=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d9dbf-1bea-41e5-87f7-9246b48b235d",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d7636fd-a91e-4b88-9e01-a763186ef054",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  base_url = base_url,\n",
    "  api_key = api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ac25f-7d1a-4bc1-92b9-6db90b268b10",
   "metadata": {},
   "source": [
    "## Use LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e96e1ea-a3dc-4bae-b131-fc34160f6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"openai/gpt-oss-20b\",\n",
    "  messages=[\n",
    "      {\n",
    "          \"role\":\"user\",\n",
    "          \"content\":\"Hi there! This is FRED.\"\n",
    "      }\n",
    "  ],\n",
    "  temperature=temperature,\n",
    "  top_p=top_p,\n",
    "  max_tokens=max_tokens,\n",
    "  stream=stream,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8b4f60-c62d-456c-a6bb-2bf436df3221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user says: \"Hi there! This is FRED.\" That is maybe a prompt from the user: \"Hi there! This is FRED.\" The user might be telling us identity: \"This is FRED.\" That might be a short conversation.\n",
      "\n",
      "We need to respond in a friendly way. The user hasn't asked a question. Maybe we should respond accordingly. We could say: \"Hello, FRED! How can I help you today?\" Potentially mention that I'm ChatGPT. Also, since it's a new conversation, we might ask them what they would like to discuss.\n",
      "\n",
      "Given something simple is fine. No constraints. Just respond.\n",
      "\n",
      "But, as ChatGPT, I might ask: \"Hi FRED! How can I assist you?\" Possibly give some useful info.\n",
      "\n",
      "But maybe the context: There's a default environment variable that the assistant's name is \"ChatGPT\". The user says \"This is FRED.\" Could be like the user is introducing a role? Or maybe they're stating their username?\n",
      "\n",
      "Would be better to respond politely: \"Hi FRED! Nice to meet you. What can I help you with today?\"\n",
      "\n",
      "We can mention that we are an AI language model and can answer a variety of queries. Or we can ask if they have any tasks or questions.\n",
      "\n",
      "Also, maybe we can actually ask: \"Welcome! How can I assist you with your work today?\"\n",
      "\n",
      "We should keep to the style guidelines for clarity: the answer is short.\n",
      "\n",
      "We might reflect the new context, but we will keep it friendly. There's no special constraints. So let's do a short friendly greeting and ask how we can help.\n",
      "\n",
      "Thus: \"Hello, FRED! I'm ChatGPT, an AI assistant. How may I help you today?\"Hi FRED! ðŸ‘‹ Iâ€™m ChatGPT, here to help with whatever you need. Whatâ€™s on your mind today?"
     ]
    }
   ],
   "source": [
    "for chunk in completion:\n",
    "  reasoning = getattr(chunk.choices[0].delta, \"reasoning_content\", None)\n",
    "  if reasoning:\n",
    "    print(reasoning, end=\"\")\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb1994-6d88-4382-9b27-21687e3d031c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
